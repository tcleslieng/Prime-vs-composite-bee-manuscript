---
title: "Exploring the discrimination of prime and composite quantities through perceptual grouping of symmetric and asymmetric element subsets by honeybees"
author: "Leslie Ng and Scarlett Howard"
date: "25-07-2024"
output:
  html_document:
   toc: yes
   toc_float: yes
   collapsed: true 
   smooth_scroll: true
   depth: 3 
   highlight: tango # different theme for the appearance of the code
   theme: flatly
   code_folding: hide
self_contained: yes
mode: selfcontained
editor_options:
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../Output") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# **Introduction**


In this study we tested if free-flying honey bees that are primed to even composite numbers spontaneously discriminate an odd prime number from an odd composite number. This document provides details on data processing, statistical analysis and figures of the original manuscript submitted for peer review.

The following code is written in the R programming language.


EE = Equal element size condition, 
SA = Equal surface area condition


# **Libraries** 

### *Install packages*

```{r eval=FALSE}
install.packages("lme4")  # For fitting models
install.packages("ggplot2") # For plotting
install.packages("wesanderson") # Color palette
install.packages("dplyr") # Data processing
install.packages("tidyverse") # QoL
install.packages("ggpubr") # Create figure
install.packages("patchwork") # Create figure
install.packages("DHARMa")
install.packages("performance") # For assumption checking
install.packages("lmerTest")
```

### *Load packages*

```{r, message=FALSE, warning=FALSE}
library(lme4) 
library(lmerTest)
library(ggplot2) 
library(wesanderson)
library(dplyr) 
library(tidyverse) 
library(patchwork) 
library(performance) 
library(DHARMa) 
```

# **Data importing**

```{r, message=FALSE, results="hide"}

PRIMEDATA <- read.csv("../Data/prime_data.csv", header=TRUE)

PRIMEGRP <- read.csv(
  "../Data/primegrouped_data.csv", header=TRUE)

CONTROL <- read.csv(
  "../Data/control_data.csv", header=TRUE)

TRAINING <- read.csv(
  "../Data/training_data.csv", header=TRUE)

TRAINING_trainingphase <- read.csv(
  "../Data/training_data_learningphase.csv", header=TRUE)

EXT_PRIMING <- read.csv(
  "../Data/60priming.csv", header=TRUE)

EXT_TRAINING <- read.csv(
  "../Data/60training_testphase_data.csv", header=TRUE)

EXT_TRAINING_trainingphase <- read.csv(
  "../Data/60training_trainphase_data.csv", header=TRUE)

#Subset Experiment 1

PRIMEDATA$BEEID <- as.factor(PRIMEDATA$BEEID) # Treat Bee ID as factor
POSITIVE <- subset(PRIMEDATA, CONDITION == "POSITIVE") 
NEGATIVE <- subset(PRIMEDATA, CONDITION == "NEGATIVE") 
POS7v9 <-subset(POSITIVE, TEST == "7v9") 
POS11v9 <-subset(POSITIVE, TEST == "11v9") 
POS13v15 <-subset(POSITIVE, TEST == "13v15") 
NEG7v9 <-subset(NEGATIVE, TEST == "7v9") 
NEG11v9 <-subset(NEGATIVE, TEST == "11v9") 
NEG13v15 <-subset(NEGATIVE, TEST == "13v15") 



#Subset Experiment 2

EXT_7v9 <- subset(EXT_PRIMING, TEST == "7v9") # 
EXT_9v11 <- subset(EXT_PRIMING, TEST == "11v9") 
EXT_13v15 <- subset(EXT_PRIMING, TEST == "13v15") 

#Subset Experiment 3

PRIMEGRP$BEEID <- as.factor(PRIMEGRP$BEEID) # Treat Bee ID as factor
POSITIVE_GRP <- subset(PRIMEGRP, CONDITION == "POS") 
NEGATIVE_GRP <- subset(PRIMEGRP, CONDITION == "NEG") 
POS7v9_GRP <-subset(POSITIVE_GRP, TEST == "7v9") 
POS11v9_GRP <-subset(POSITIVE_GRP, TEST == "11v9") 
POS13v15_GRP <-subset(POSITIVE_GRP, TEST == "13v15") 
NEG7v9_GRP <-subset(NEGATIVE_GRP, TEST == "7v9") 
NEG11v9_GRP <-subset(NEGATIVE_GRP, TEST == "11v9") 
NEG13v15_GRP <-subset(NEGATIVE_GRP, TEST == "13v15") 
POSCONT_GRP <-subset(POSITIVE_GRP, TEST == "control")
NEGCONT_GRP<-subset(NEGATIVE_GRP, TEST == "control")
summary(POS7v9_GRP)
summary(NEG7v9_GRP)
summary(POS11v9_GRP)
summary(NEG11v9_GRP)
summary(POS13v15_GRP)
summary(NEG13v15_GRP)


#Subset control experiment 4 data

CONTROL$BEEID <- as.factor(CONTROL$BEEID) # Treat Bee ID as factor

conSA <-subset(CONTROL, CONDITION == "SA") 
conEE <-subset(CONTROL, CONDITION == "EE") 

EEcon3v1 <-subset(conEE, TEST == "3v1") 
EEcon3v2 <-subset(conEE, TEST == "3v2") 
EEcon3v4 <-subset(conEE, TEST == "3v4") 
EEcon3v5 <-subset(conEE, TEST == "3v5") 

SAcon3v1 <-subset(conSA, TEST == "3v1") 
SAcon3v2 <-subset(conSA, TEST == "3v2") 
SAcon3v4 <-subset(conSA, TEST == "3v4") 
SAcon3v5 <-subset(conSA, TEST == "3v5") 

summary(EEcon3v1)
summary(EEcon3v2)
summary(EEcon3v4)
summary(EEcon3v5)
summary(SAcon3v1)
summary(SAcon3v2)
summary(SAcon3v4)
summary(SAcon3v5)

#Subset Experiment 5 training data

TRAINING$BEEID <- as.factor(TRAINING$BEEID) # Treat Bee ID as factor
TRAINING$CONDITION <- as.factor(TRAINING$CONDITION)
summary(TRAINING)


Training_learningtest <- subset(TRAINING, CONDITION == "Learning")
Training_learning_EE <- subset(TRAINING, STIMULI == "EE")
Training_learning_SA <- subset(TRAINING, STIMULI == "SA")
Training_7v9test <- subset(TRAINING, CONDITION == "7v9") 
Training_7v8test <-subset(TRAINING, CONDITION == "7v8")  

Training_learningtest_PRIME <- subset(Training_learningtest, PRIMECOMP == "PRIME")
Training_7v9test_PRIME <- subset(Training_7v9test, PRIMECOMP == "PRIME") 
Training_7v8test_PRIME <-subset(Training_7v8test, PRIMECOMP == "PRIME")  
Training_learningtest_COMPOSITE <- subset(Training_learningtest, PRIMECOMP == "COMPOSITE")
Training_7v9test_COMPOSITE <- subset(Training_7v9test, PRIMECOMP == "COMPOSITE") 
Training_7v8test_COMPOSITE <-subset(Training_7v8test, PRIMECOMP == "COMPOSITE")  

summary(Training_7v9test)
summary(Training_7v8test)
summary(Training_learningtest)

summary(TRAINING)
summary(Training_learning_EE)
summary(Training_learning_SA)

#Subset Experiment 6

EXT2_LEARNING <- subset(EXT_TRAINING, TEST == "Learning") 
EXT2_7v9 <- subset(EXT_TRAINING, TEST == "7v9") 
EXT2_7v8 <- subset(EXT_TRAINING, TEST == "7v8") 

EXT2_LEARNING_PRIME <- subset(EXT2_LEARNING, CONDITION == "PRIME")
EXT2_7v9_PRIME <- subset(EXT2_7v9, CONDITION == "PRIME") 
EXT2_7v8_PRIME <-subset(EXT2_7v8, CONDITION == "PRIME")  

EXT2_LEARNING_COMPOSITE <- subset(EXT2_LEARNING, CONDITION == "COMPOSITE")
EXT2_7v9_COMPOSITE <- subset(EXT2_7v9, CONDITION == "COMPOSITE") 
EXT2_7v8_COMPOSITE <-subset(EXT2_7v8, CONDITION == "COMPOSITE")  


summary(POS7v9)
summary(NEG7v9)
summary(POS11v9)
summary(NEG11v9)
summary(POS13v15)
summary(NEG13v15)
summary(EXT_7v9)
summary(EXT_9v11)
summary(EXT_13v15)
summary(EXT2_LEARNING)
summary(EXT2_7v9)
summary(EXT2_7v8)


```

# **Generalized linear models**

## Experiment 1

```{r message=FALSE, results='hide', error=FALSE}

lm1 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POS7v9) 
summary(lm1)
lm2 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POS11v9) 
summary(lm2)
lm3 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POS13v15) 
summary(lm3)
lm4 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEG7v9) 
summary(lm4)
lm5 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEG11v9) 
summary(lm5)
lm6 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEG13v15) 
summary(lm6)

```

### *Pooled dataset* 

We use and report models using a pooled dataset (combining equal element size and equal surface area groups) as we found no statistically significant results in either condition

```{r message=FALSE, error=FALSE}
 
EXP1POOLED7v9 <-subset(PRIMEDATA, TEST == "7v9") 
EXP1POOLED11v9 <-subset(PRIMEDATA, TEST == "11v9") 
EXP1POOLED13v15 <-subset(PRIMEDATA, TEST == "13v15") 

lm7 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP1POOLED7v9) 
summary(lm7)
lm8 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP1POOLED11v9) 
summary(lm8)
lm9 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP1POOLED13v15) 
summary(lm9)

```

### *Confidence intervals*

```{r results='hide', message=FALSE, error=FALSE, warning=FALSE}

confint(lm7)
EXP1_7v9confintlow <-(exp(-0.3941168)/(1 + exp( -0.3941168)))
EXP1_7v9confinthigh <-(exp(0.5825354)/(1 + exp(0.5825354)))
confint(lm8)
EXP1_11v9confintlow <-(exp(-0.1001439)/(1 + exp(-0.1001439)))
EXP1_11v9confinthigh <-(exp(0.5160843)/(1 + exp(0.5160843)))
confint(lm9)
EXP1_13v15confintlow <-(exp(-0.3178457)/(1 + exp(-0.3178457)))
EXP1_13v15confinthigh <-(exp(0.2373101)/(1 + exp(0.2373101)))

```

## Experiment 2

```{r message=FALSE, error=FALSE}
lm_ext1 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT_7v9) 
summary(lm_ext1)
lm_ext2 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT_9v11) 
summary(lm_ext2)
lm_ext3 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT_13v15) 
summary(lm_ext3)

```

### *Confidence intervals*

```{r results='hide', error=FALSE, message=FALSE, warning=FALSE}
confint(lm_ext1)
ext1_confintlow <-(exp(-0.5754465)/(1 + exp(-0.5754465)))
ext1_confinthigh <-(exp(0.2440522)/(1 + exp(0.2440522)))

confint(lm_ext2)
ext2_confintlow <-(exp(-0.3690665)/(1 + exp(-0.3690665)))
ext2_confinthigh <-(exp(0.4516763)/(1 + exp(0.4516763)))

confint(lm_ext3)
ext3_confintlow <-(exp(-0.1523864)/(1 + exp(-0.1523864)))
ext3_confinthigh <-(exp(0.6439017)/(1 + exp(0.6439017)))
```

## Experiment 3

```{r results='hide', error=FALSE, message=FALSE}

lm10 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POS7v9_GRP) 
summary(lm10)
lm11 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POS11v9_GRP) 
summary(lm11)
lm12 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POS13v15_GRP) 
summary(lm12)
lm13 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = POSCONT_GRP) 
summary(lm13)

lm14 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEG7v9_GRP) 
summary(lm14)
lm15 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEG11v9_GRP) 
summary(lm15)
lm16 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEG13v15_GRP) 
summary(lm16)
lm17 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = NEGCONT_GRP) 
summary(lm17)

```

### *Pooled dataset*

We use and report models using a pooled dataset (combining equal element size and equal surface area groups) as we found no statistically significant results in either condition

```{r message=FALSE, error=FALSE}

EXP2POOLED7v9 <-subset(PRIMEGRP, TEST == "7v9") 
EXP2POOLED11v9 <-subset(PRIMEGRP, TEST == "11v9") 
EXP2POOLED13v15 <-subset(PRIMEGRP, TEST == "13v15") 
EXP2POOLEDNEGCONT <-subset(PRIMEGRP, TEST == "control") 

lm18 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP2POOLED7v9) 
summary(lm18)
lm19 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP2POOLED11v9) 
summary(lm19)
lm20 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP2POOLED13v15) 
summary(lm20)
lm21<-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXP2POOLEDNEGCONT) 
summary(lm21)

```

### *Confidence intervals*

```{r results='hide', error=FALSE, warning=FALSE, message=FALSE}

confint(lm18)
EXP2_7v9confintlow <-(exp(-0.04782218)/(1 + exp(-0.04782218)))
EXP2_7v9confinthigh <-(exp(0.5434547)/(1 + exp(0.5434547)))
confint(lm19)
EXP2_11v9confintlow <-(exp(-0.197514)/(1 + exp(-0.197514)))
EXP2_11v9confinthigh <-(exp(0.3590998)/(1 + exp(0.3590998)))
confint(lm20)
EXP2_13v15confintlow <-(exp(-0.4808859)/(1 + exp(-0.4808859)))
EXP2_13v15confinthigh <-(exp(0.07708543)/(1 + exp(0.07708543)))
confint(lm21)
EXP2_NEGCONTconfintlow <-(exp(-0.3787003)/(1 + exp(-0.3787003)))
EXP2_NEGCONTconfinthigh <-(exp(0.1772473)/(1 + exp(0.1772473)))

```

## Control experiment 4

```{r message=FALSE, error=FALSE}

clm1 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EEcon3v1) 
summary(clm1)
clm2 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EEcon3v2) 
summary(clm2)
clm3 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EEcon3v4) 
summary(clm3)
clm4 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EEcon3v5) 
summary(clm4)
clm5 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = SAcon3v1) 
summary(clm5)
clm6 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = SAcon3v2) 
summary(clm6)
clm7 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = SAcon3v4) 
summary(clm7)
clm8 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = SAcon3v5) 
summary(clm8)


```

### *Confidence intervals*


```{r results='hide', error=FALSE, message=FALSE, warning=FALSE}

confint(clm1)
EE3v1confintlow <-(exp(0.3592665)/(1 + exp(0.3592665)))
EE3v1confinthigh <-(exp(1.2555766)/(1 + exp(1.2555766)))
confint(clm2)
EE3v2confintlow <-(exp(-0.632105)/(1 + exp(-0.632105)))
EE3v2confinthigh <-(exp(0.6466399)/(1 + exp(0.6466399)))
confint(clm3)
EE3v4confintlow <-(exp(-0.408491)/(1 + exp(-0.408491)))
EE3v4confinthigh <-(exp(0.5026474)/(1 + exp(0.5026474)))
confint(clm4)
EE3v5confintlow <-(exp(-0.3296674)/(1 + exp(-0.3296674)))
EE3v5confinthigh <-(exp(0.7213893)/(1 + exp(0.7213893)))

confint(clm5)
SA3v1confintlow <-(exp(-0.1827046)/(1 + exp(-0.1827046)))
SA3v1confinthigh <-(exp(0.7875859)/(1 + exp(0.7875859)))
confint(clm6)
SA3v2confintlow <-(exp(-0.6834336)/(1 + exp(-0.6834336)))
SA3v2confinthigh <-(exp(0.7978965)/(1 + exp(0.7978965)))
confint(clm7)
SA3v4confintlow <-(exp(-0.2441276)/(1 + exp(-0.2441276)))
SA3v4confinthigh <-(exp(0.5751515)/(1 + exp(0.5751515)))
confint(clm8)
SA3v5confintlow <-(exp(-0.6521079)/(1 + exp(-0.6521079)))
SA3v5confinthigh <-(exp(0.3092899)/(1 + exp(0.3092899)))
```

## Training experiment 5 (30 trials)

```{r message=FALSE, error=FALSE}
# Test phase
tlm1 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_learningtest) 
summary(tlm1)
tlm2 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_7v9test) 
summary(tlm2)
tlm3 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_7v8test) 
summary(tlm3)
tlm_stimuli <- glmer(CHOICE~ STIMULI + (1|BEEID), family = binomial, data = Training_learningtest) 
summary(tlm_stimuli)
tlm_stimuli2 <- glmer(CHOICE~ STIMULI + (1|BEEID), family = binomial, data = Training_7v9test) 
summary(tlm_stimuli)
tlm_stimuli3 <- glmer(CHOICE~ STIMULI + (1|BEEID), family = binomial, data = Training_7v8test) 
summary(tlm_stimuli)



lm_ext4_prime <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_learningtest_PRIME) 
summary(lm_ext4_prime)
lm_ext5_prime <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_7v9test_PRIME) 
summary(lm_ext5_prime)
lm_ext6_prime <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_7v8test_PRIME) 
summary(lm_ext6_prime)

lm_ext4_composite <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_learningtest_COMPOSITE) 
summary(lm_ext4_composite)
lm_ext5_composite <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_7v9test_COMPOSITE) 
summary(lm_ext5_composite)
lm_ext6_composite <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_7v8test_COMPOSITE) 
summary(lm_ext6_composite)
mean(Training_7v8test_COMPOSITE$CHOICE)
mean(Training_7v8test_PRIME$CHOICE)


# Learning phase

tlm5 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_learning_EE) 
tlm6 <- glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = Training_learning_SA) 
summary(tlm5)
summary(tlm6)

lm_trainingphase1 <-glmer(CHOICE~ CNUM + (1|BEEID), family = binomial, data = TRAINING_trainingphase) 
summary(lm_trainingphase1)
lm_trainingphase2 <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = TRAINING_trainingphase) 
summary(lm_trainingphase2)
mean(TRAINING_trainingphase$CHOICE)
lm_trainphase3 <-glmer(CHOICE~ PRIMECOMP + (1|BEEID), family = binomial, data = TRAINING_trainingphase) 
summary(lm_trainphase3)

```

### *Confidence intervals*

```{r results='hide', error=FALSE, message=FALSE, warning=FALSE}

confint(tlm1)
traininglearningconfintlow <-(exp(-0.311725)/(1 + exp(-0.311725)))
traininglearningconfinthigh <-(exp(0.3117250)/(1 + exp(0.3117250)))
confint(tlm2)
training7v9confintlow <-(exp(-0.1166512)/(1 + exp(-0.1166512)))
training7v9confinthigh <-(exp(0.5277194)/(1 + exp(0.5277194)))
confint(tlm3)
training7v8confintlow <-(exp(-0.2557725)/(1 + exp(-0.2557725)))
training7v8confinthigh <-(exp(0.5267269)/(1 + exp(0.5267269)))
confint(tlm5)
training_stim_confintlow <-(exp(-0.7246346)/(1 + exp(-0.7246346)))
training_stim_confinthigh <-(exp(0.5220422)/(1 + exp(0.5220422)))

confint(lm_ext6_composite)
ext6_composite_confintlow <-(exp(-0.08221807)/(1 + exp(-0.08221807)))
ext6_composite_confinthigh <-(exp(1.298001)/(1 + exp(1.298001)))
print(ext6_composite_confintlow)
print(ext6_composite_confinthigh)

confint(lm_ext6_prime)
ext6_prime_confintlow <-(exp(-0.7150887)/(1 + exp(-0.7150887)))
ext6_prime_confinthigh <-(exp(0.1958676)/(1 + exp(0.1958676)))
print(ext6_prime_confintlow)
print(ext6_prime_confinthigh)


confint(tlm5)
training_EE_confintlow <-(exp(-0.06959618)/(1 + exp(-0.06959618)))
training_EE_confinthigh <-(exp(0.4391975)/(1 + exp(0.4391975)))
confint(tlm6)
training_SA_confintlow <-(exp(-0.3101593)/(1 + exp(-0.3101593)))
training_SA_confinthigh <-(exp(0.3813845)/(1 + exp(0.3813845)))


confint(lm_trainingphase1)
trainingphaseCNUMconfintlow <-(exp(-0.002829895)/(1 + exp(-0.002829895)))
trainingphaseCNUMconfinthigh <-(exp(0.03983277)/(1 + exp(0.03983277)))
print(trainingphaseCNUMconfintlow)
print(trainingphaseCNUMconfinthigh)

confint(lm_trainingphase2)
trainingphase2confintlow <-(exp(0.2581226)/(1 + exp(0.2581226)))
trainingphase2confinthigh <-(exp(0.6249301)/(1 + exp(0.6249301)))
print(trainingphase2confintlow)
print(trainingphase2confinthigh)
```

## Training experiments 6 (60 trials)

```{r message=FALSE, error=FALSE}
# Test phase

lm_ext4 <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_LEARNING) 
summary(lm_ext4)
lm_ext5 <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_7v9) 
summary(lm_ext5)
lm_ext6 <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_7v8) 
summary(lm_ext6)

lm_ext4_prime <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_LEARNING_PRIME) 
summary(lm_ext4_prime)
lm_ext5_prime <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_7v9_PRIME) 
summary(lm_ext5_prime)
lm_ext6_prime <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_7v8_PRIME) 
summary(lm_ext6_prime)

lm_ext4_composite <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_LEARNING_COMPOSITE) 
summary(lm_ext4_composite)
lm_ext5_composite <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_7v9_COMPOSITE) 
summary(lm_ext5_composite)
lm_ext6_composite <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT2_7v8_COMPOSITE) 
summary(lm_ext6_composite)

mean(EXT2_LEARNING_PRIME$CHOICE)
mean(EXT2_LEARNING_COMPOSITE$CHOICE)
mean(EXT2_7v8_COMPOSITE$CHOICE)
mean(EXT2_7v8_PRIME$CHOICE)
mean(EXT2_7v9_COMPOSITE$CHOICE)
mean(EXT2_7v9_PRIME$CHOICE)

# Training phase

lm_ext_training <-glmer(CHOICE~ CNUM + (1|BEEID), family = binomial, data = EXT_TRAINING_trainingphase) 
summary(lm_ext_training)

lm_ext_training2 <-glmer(CHOICE~ 1 + (1|BEEID), family = binomial, data = EXT_TRAINING_trainingphase) 
summary(lm_ext_training2)
mean(EXT_TRAINING_trainingphase$CHOICE)

lm_ext_training_condition <-glmer(CHOICE~ CONDITION + (1|BEEID), family = binomial, data = EXT_TRAINING_trainingphase) 
summary(lm_ext_training_condition)
```

### *Confidence intervals*

```{r results='hide', error=FALSE, message=FALSE, warning=FALSE}
confint(lm_ext4)
ext4_confintlow <-(exp(0.1115877)/(1 + exp(0.1115877)))
ext4_confinthigh <-(exp(0.8494996)/(1 + exp(0.8494996)))

confint(lm_ext5)
ext5_confintlow <-(exp(-0.1911968)/(1 + exp(-0.1911968)))
ext5_confinthigh <-(exp(0.5289128)/(1 + exp(0.5289128)))

confint(lm_ext6)
ext6_confintlow <-(exp(-0.06882467)/(1 + exp(-0.06882467)))
ext6_confinthigh <-(exp(0.6940668)/(1 + exp(0.6940668)))

confint(lm_ext_training)
trainingphase_extconfintlow <-(exp(-0.002982056)/(1 + exp(-0.002982056)))
trainingphase_extconfinthigh <-(exp(0.0142990)/(1 + exp(0.0142990)))
print(trainingphase_extconfintlow)
print(trainingphase_extconfinthigh)

confint(lm_ext_training2)
trainingphase2_extconfintlow <-(exp(0.2627744)/(1 + exp(0.2627744)))
trainingphase2_extconfinthigh <-(exp(0.5612588)/(1 + exp(0.5612588)))
print(trainingphase2_extconfintlow)
print(trainingphase2_extconfinthigh)

confint(lm_ext4_prime)
ext4_primeconfintlow <-(exp(0.03109655)/(1 + exp(0.03109655)))
ext4_primeconfinthigh <-(exp(1.0915254)/(1 + exp(1.0915254)))

confint(lm_ext4_composite)
ext4_compositeconfintlow <-(exp(-0.1256052)/(1 + exp(-0.1256052)))
ext4_compositeconfinthigh <-(exp(0.9778516)/(1 + exp(0.9778516)))

confint(lm_ext5_prime)
ext5_primeconfintlow <-(exp(-0.1896967)/(1 + exp(-0.1896967)))
ext5_primeconfinthigh <-(exp(0.7046346)/(1 + exp(0.7046346)))

confint(lm_ext6_composite)
ext6_compositeconfintlow <-(exp(-0.08221807)/(1 + exp(-0.08221807)))
ext6_compositeconfinthigh <-(exp(1.298001)/(1 + exp(1.298001)))

confint(lm_ext6_prime)
ext6_primeconfintlow <-(exp(-0.4444105)/(1 + exp(-0.4444105)))
ext6_primeconfinthigh <-(exp(0.5816044)/(1 + exp(0.5816044)))

```


# **Figures**

## Dataframes

```{r message=FALSE, error=FALSE}

# Create dataframe for experiment 1 bargraph

dframe <- data.frame(Test = rep(c("7v9", "11v9", "13v15")),
                     percentage = c(0.52,0.55,0.49),
                     confintlow = c(EXP1_7v9confintlow,EXP1_11v9confintlow,EXP1_13v15confintlow),
                     confinthigh = c(EXP1_7v9confinthigh,EXP1_11v9confinthigh,EXP1_13v15confinthigh))


exp1points <- PRIMEDATA %>%   # calculate mean of each block for each bee
  group_by(BEEID, TEST, CONDITION) %>% 
  summarize(prop = mean(CHOICE)) 

# Create dataframe for Experiment 2
summary(EXT_7v9)
summary(EXT_9v11)
summary(EXT_13v15)

dframeext1 <- data.frame(Test = rep(c("7v9", "11v9", "13v15")),
                     percentage = c(0.46,0.51,0.56),
                     confintlow = c(ext1_confintlow,ext2_confintlow,ext3_confintlow),
                     confinthigh = c(ext1_confinthigh,ext2_confinthigh,ext3_confinthigh))


ext1points <- EXT_PRIMING %>%   # calculate mean of each block for each bee
  group_by(BEEID, TEST) %>% 
  summarize(prop = mean(CHOICE)) 


# Create dataframe for experiment 3 bargraph

dframe2 <- data.frame(Test = rep(c("7v9", "11v9", "13v15","control")), 
                     percentage = c(0.56,0.52,0.45,0.475),
                     confintlow = c(EXP2_7v9confintlow,EXP2_11v9confintlow,EXP2_13v15confintlow,EXP2_NEGCONTconfintlow),
                     confinthigh = c(EXP2_7v9confinthigh,EXP2_11v9confinthigh,EXP2_13v15confinthigh,EXP2_NEGCONTconfinthigh))

exp2points <- PRIMEGRP %>%   # calculate mean of each block for each bee
  group_by(BEEID, TEST, CONDITION) %>% 
  summarize(prop = mean(CHOICE))


#Create dataframe for control experiment 4 bargraph

dframecon <- data.frame(Test = rep(c("3v1", "3v2","3v4","3v5"), each = 2), 
                    Condition = rep(c("EE", "SA"), times = 4),
                    percentage = c(0.6889, 0.57, 0.5, 0.51, 0.5111, 0.54, 0.5444, 0.46),
                     confintlow = c(EE3v1confintlow, SA3v1confintlow, EE3v2confintlow, SA3v2confintlow, EE3v4confintlow, SA3v4confintlow, EE3v5confintlow, SA3v5confintlow),
                     confinthigh = c(EE3v1confinthigh, SA3v1confinthigh, EE3v2confinthigh, SA3v2confinthigh, EE3v4confinthigh, SA3v4confinthigh, EE3v5confinthigh, SA3v5confinthigh))


controlpoints <- CONTROL %>%   # calculate mean of each block for each bee
  group_by(BEEID, TEST, CONDITION) %>% 
  summarize(prop = mean(CHOICE)) 



# Create dataframe for Experiment 5

dframetrn <- data.frame(Test = rep(c("Learning", "7v9","7v8P", "7v8C")), #create dataframe
                        percentage = c(0.5, 0.55, 0.44,0.63),
                        confintlow = c(traininglearningconfintlow, training7v9confintlow, ext6_prime_confintlow, ext6_composite_confintlow),
                        confinthigh = c(traininglearningconfinthigh, training7v9confinthigh, ext6_prime_confinthigh, ext6_composite_confinthigh))

trainingpoints <- TRAINING %>%
  mutate(CONDITION2 = case_when(
    CONDITION == "7v8" & PRIMECOMP == "PRIME" ~ "7v8P",
    CONDITION == "7v8" & PRIMECOMP == "COMPOSITE" ~ "7v8C",
    TRUE ~ CONDITION  # keep other conditions the same
  )) %>%
  group_by(BEEID, CONDITION2) %>%
  summarize(prop = mean(CHOICE), .groups = "drop")


levels(trainingpoints$CONDITION2) # may still need ordering if it's a factor


trainingpoints$CONDITION2 <- factor(trainingpoints$CONDITION2, 
                                    levels = c("Learning", "7v9", "7v8P", "7v8C"))
levels(trainingpoints$CONDITION2)

dframetrn$Test <- factor(dframetrn$Test, levels = dframetrn$Test)


trainingpoints$CONDITION2 <- factor(trainingpoints$CONDITION2, levels = levels(dframetrn$Test))


# Create dataframe for Experiment 6

summary(EXT2_LEARNING)
summary(EXT2_7v9)
summary(EXT2_7v8)

dframeext2 <- data.frame(Test = rep(c("LearningP", "LearningC", "7v9","7v8P", "7v8C")), #create dataframe
                        percentage = c(0.63, 0.60, 0.54, 0.52, 0.63),
                        confintlow = c(ext4_primeconfintlow, ext4_compositeconfintlow, ext5_confintlow, ext6_primeconfintlow, ext6_compositeconfintlow),
                        confinthigh = c(ext4_primeconfinthigh, ext4_compositeconfinthigh, ext5_confinthigh, ext6_primeconfinthigh, ext6_compositeconfinthigh))

dframeext2$Test <- factor(dframeext2$Test, levels=c('LearningP','LearningC', '7v9', '7v8P','7v8C'))

ext2points <- EXT_TRAINING %>%
  mutate(TEST2 = case_when(
    TEST == "Learning" & CONDITION == "PRIME" ~ "LearningP",
    TEST == "Learning" & CONDITION == "COMPOSITE" ~ "LearningC",
    TEST == "7v8" & CONDITION == "PRIME" ~ "7v8P",
    TEST == "7v8" & CONDITION == "COMPOSITE" ~ "7v8C",
    TRUE ~ TEST  # keep other conditions the same
  )) %>%
  group_by(BEEID, TEST2) %>%
  summarize(prop = mean(CHOICE), .groups = "drop")


ext2points$TEST2 <- factor(ext2points$TEST2, levels = dframeext2$Test)



# Create dataframe for supplementary figure comparing performance in EE and SA

dframetrn2 <- data.frame(Stimuli = rep(c("EE", "SA")), #create dataframe
                        percentage = c(0.55, 0.51),
                        confintlow = c(training_EE_confintlow, training_SA_confintlow),
                        confinthigh = c(training_EE_confinthigh, training_SA_confinthigh))

trainingpoints2 <- Training_learningtest %>%   # calculate mean of each block for each bee
  group_by(BEEID, STIMULI) %>% 
  summarize(prop = mean(CHOICE))


```

## GGplot

```{r message=FALSE, error=FALSE}
           
# Generate bargraph for experiment 1 

exp1graph <- ggplot()+
  labs (x= "Test condition", y = "Mean percentage of choices") +
  geom_bar(data = dframe, aes(x = Test, y = percentage, fill = Test), stat = "identity", position = "dodge") + 
   geom_point(data = exp1points, aes(x = TEST, y = prop, col = BEEID), position = position_jitter(width=0.15, height=0.01), size =2) +
  geom_errorbar(data= dframe, aes(x = Test, y = percentage, ymin = confintlow, ymax = confinthigh), width=.2, position=position_dodge(.9)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", colour = " black") +
  scale_fill_manual(values = wes_palette("GrandBudapest2", n = 3)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL)) +
  scale_x_discrete(labels=c("7v9" = "7 vs 9", "11v9" = "11 vs 9", "13v15" = "13 vs 15"))+
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1/1, 
        plot.margin = margin(0.3,0.3,0.3,0.3, "cm"),
        axis.title.x = element_text(vjust = -3)) + theme(legend.position = "none")
print(exp1graph)

# Generate bargraph for experiment 

ext1graph <- ggplot()+
  labs (x= "Test condition", y = "Mean percentage of choices") +
  geom_bar(data = dframeext1, aes(x = Test, y = percentage, fill = Test), stat = "identity", position = "dodge") + 
   geom_point(data = ext1points, aes(x = TEST, y = prop, col = BEEID), position = position_jitter(width=0.15, height=0.01), size =2) +
  geom_errorbar(data= dframeext1, aes(x = Test, y = percentage, ymin = confintlow, ymax = confinthigh), width=.2, position=position_dodge(.9)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", colour = " black") +
  scale_fill_manual(values = wes_palette("GrandBudapest2", n = 3)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL), limits = c(0, 1)) +
  scale_x_discrete(labels=c("7v9" = "7 vs 9", "11v9" = "11 vs 9", "13v15" = "13 vs 15"))+
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1/1, 
        plot.margin = margin(0.3,0.3,0.3,0.3, "cm"),
        axis.title.x = element_text(vjust = -3)) + theme(legend.position = "none")
print(ext1graph)

# Generate bargraph for experiment 3
exp3graph <- ggplot() +
  labs(x = "Test condition", y = "Mean percentage of choices") +  # Set y-axis title to NULL
  geom_bar(data = dframe2, aes(x = Test, y = percentage, fill = Test), stat = "identity", position = "dodge") + 
  geom_point(data = exp2points, aes(x = TEST, y = prop, col = BEEID), position = position_jitter(width = 0.15, height=0.01), size = 2) +
  geom_errorbar(data = dframe2, aes(x = Test, y = percentage, ymin = confintlow, ymax = confinthigh), width = .2, position = position_dodge(.9)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", colour = "black") +
  scale_fill_manual(values = wes_palette("GrandBudapest2", n = 4)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL)) +
  scale_x_discrete(labels = c("7v9" = "7 vs 9", "11v9" = "11 vs 9", "13v15" = "13 vs 15", "control" = "control")) +
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1 / 1, 
        plot.margin = margin(0.3,0.3,0.3,0.3, "cm"),
        axis.title.x = element_text(vjust = -3)) + theme(legend.position = "none")

print(exp3graph)

# Generate bargraph for control experiment 4

custom_shapes <- c(21, 22, 23, 24, 25, 21, 22, 23, 24, 25)

congraph <- ggplot()+ 
  labs (x= "Test condition", y = "Mean percentage of choices") +
  geom_bar(data = dframecon, aes(x = Test, y = percentage, fill = Condition), stat = "identity", position = "dodge") + 
  geom_hline(yintercept = 0.5, linetype = "dashed", colour = " black") +
  geom_point(data = controlpoints, aes(x = TEST, y = prop, fill = CONDITION), position = position_jitterdodge(jitter.width=0.15, jitter.height = 0.01, dodge.width = 0.9), show.legend = FALSE, size =1, colour="#696969") +
  scale_fill_manual(values = wes_palette("GrandBudapest2", n = 2)) +
  geom_errorbar(data= dframecon, aes(x = Test, y = percentage, ymin = confintlow, ymax = confinthigh, group = Condition), width=.15, position=position_dodge(.9)) +
  geom_text(data= dframecon, x =0.775, y = 0.83, label ="*", size =6) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL))+
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1/1,
        axis.title.x = element_text(vjust = -3),
        plot.margin = margin(0.3,0.3,0.3,0.3, "cm")) + theme(legend.position = "none")
print(congraph)

# Generate bargraph for training experiment 5

trngraph <- ggplot()+
  labs (x= "Test condition", y = "Mean percentage of choices") +
  geom_bar(data = dframetrn, aes(x = Test, y = percentage, fill = Test), stat = "identity", position = "dodge") + 
   geom_point(data = trainingpoints, aes(x = CONDITION2, y = prop, col = BEEID), position = position_jitter(width=0.15,height=0.01), size =2) +
  geom_errorbar(data= dframetrn, aes(x = Test, y = percentage, ymin = confintlow, ymax = confinthigh), width=.2, position=position_dodge(.9)) +
  geom_text(data = data.frame(Test = "7v8C", percentage = 0.95), 
            aes(x = Test, y = percentage, label = "*"), 
            size = 10, color = "black") +
    geom_hline(yintercept = 0.5, linetype = "dashed", colour = " black") +
  scale_fill_manual(values = wes_palette("Moonrise3", n = 4)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL), limits = c(0, 1)) +
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1/1, 
        plot.margin = margin(0.3,0.3,0.3,0.3, "cm"),
        axis.title.x = element_text(vjust = -3)) + theme(legend.position = "none")
print(trngraph)

# Generate bargraph for training experiment 6

ext2graph <- ggplot() +
  labs(x = "Test condition", y = "Mean percentage of choices") +
  geom_bar(data = dframeext2, aes(x = Test, y = percentage, fill = Test), stat = "identity", position = "dodge") +
  geom_point(data = ext2points, aes(x = TEST2, y = prop, col = BEEID), position = position_jitter(width = 0.15, height = 0.01), size = 2) +
  geom_errorbar(data = dframeext2, aes(x = Test, y = percentage, ymin = confintlow, ymax = confinthigh), width = 0.2, position = position_dodge(0.9)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", colour = "black") +
  geom_text(data = data.frame(Test = "LearningP", percentage = 0.9), 
            aes(x = Test, y = percentage, label = "*"), 
            size = 10, color = "black") +
  scale_fill_manual(values = wes_palette("Moonrise3", n = 5)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL), limits = c(0, 1)) +
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1/1,
        plot.margin = margin(0.3, 0.3, 0.3, 0.3, "cm"),
        axis.title.x = element_text(vjust = -3),
        legend.position = "none")

print(ext2graph)


# Generate bargraph comparing EE and SA performance in the learning test

trngraph2 <- ggplot()+
  labs (x= "Stimulus condition", y = "Mean percentage of correct choices") +
  geom_bar(data = dframetrn2, aes(x = Stimuli, y = percentage, fill = Stimuli), stat = "identity", position = "dodge") + 
   geom_point(data = trainingpoints2, aes(x = STIMULI, y = prop, col = BEEID), position = position_jitter(width=0.15, height=0.01), size =2) +
  geom_errorbar(data= dframetrn2, aes(x = Stimuli, y = percentage, ymin = confintlow, ymax = confinthigh), width=.2, position=position_dodge(.9)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", colour = " black") +
  scale_fill_manual(values = wes_palette("GrandBudapest2", n = 2)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1, suffix = NULL), limits = c(0, 1))  +
  theme(panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1/1, 
        axis.title.y = element_text(vjust = 5),
        axis.title.x = element_text(vjust = -3),
        plot.margin = margin(0.6,0.6,0.6,0.6, "cm")) + theme(axis.title = element_text(size = 15),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15),
    axis.title.y = element_text(vjust = 5, size = 15)) + theme(legend.position = "none")
print(trngraph2)

```

## Learning phase graph for 30 and 60 trials of training (Experiments 5 and 6)

```{r message=FALSE, error=FALSE}

# Experiment 5

new_data_ext <- data.frame(
  CNUM = seq(min(TRAINING_trainingphase$CNUM), max(TRAINING_trainingphase$CNUM), length.out = 100)
)

new_data_ext$BEEID <- NA

pred_ext <- predict(lm_trainingphase1, newdata = new_data_ext, type = "link", se.fit = TRUE, re.form = NA)

# Convert fit and CI to percentages
new_data_ext$fit <- plogis(pred_ext$fit) * 100
new_data_ext$lwr <- plogis(pred_ext$fit - 1.96 * pred_ext$se.fit) * 100
new_data_ext$upr <- plogis(pred_ext$fit + 1.96 * pred_ext$se.fit) * 100

p_ext <- ggplot(new_data_ext, aes(x = CNUM, y = fit)) +
  geom_line(size = 1.2, color = "steelblue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, fill = "steelblue") +
  scale_y_continuous(
    breaks = seq(0, 100, 10),
    limits = c(0, 100),
    expand = c(0, 0),
    labels = function(x) paste0(x, "%")
  ) +
  geom_hline(yintercept = 50, linetype = "dashed", colour = "black") +
  labs(y = "Predicted probability of correct choice (%)", x = "Trial number") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(size = 15, margin = margin(0, 10, 0, 0)),
    axis.title.x = element_text(size = 15, margin = margin(10, 0, 0, 0)),
    axis.text.x = element_text(size = 15, colour = "black"),
    axis.text.y = element_text(size = 15, colour = "black"),
    legend.position = "none",
    axis.line = element_line(color = "black"),
    panel.background = element_rect(fill = "white", color = NA)
  )

print(p_ext)

ggsave("p_ext5.png", plot = p_ext, width = 5, height = 5, dpi = 300, path = "../Output")
ggsave("p_ext5.pdf", plot = p_ext, width = 5, height = 5, dpi = 300, path = "../Output")


# Experiment 6

new_data_ext <- data.frame(
  CNUM = seq(min(EXT_TRAINING_trainingphase$CNUM), max(EXT_TRAINING_trainingphase$CNUM), length.out = 100)
)

new_data_ext$BEEID <- NA

pred_ext <- predict(lm_ext_training, newdata = new_data_ext, type = "link", se.fit = TRUE, re.form = NA)

# Convert fit and CI to percentages
new_data_ext$fit <- plogis(pred_ext$fit) * 100
new_data_ext$lwr <- plogis(pred_ext$fit - 1.96 * pred_ext$se.fit) * 100
new_data_ext$upr <- plogis(pred_ext$fit + 1.96 * pred_ext$se.fit) * 100

p_ext2 <- ggplot(new_data_ext, aes(x = CNUM, y = fit)) +
  geom_line(size = 1.2, color = "steelblue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, fill = "steelblue") +
  scale_y_continuous(
    breaks = seq(0, 100, 10),
    limits = c(0, 100),
    expand = c(0, 0),
    labels = function(x) paste0(x, "%")
  ) +
  geom_hline(yintercept = 50, linetype = "dashed", colour = "black") +
  labs(y = "Predicted probability of correct choice (%)", x = "Trial number") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(size = 15, margin = margin(0, 10, 0, 0)),
    axis.title.x = element_text(size = 15, margin = margin(10, 0, 0, 0)),
    axis.text.x = element_text(size = 15, colour = "black"),
    axis.text.y = element_text(size = 15, colour = "black"),
    legend.position = "none",
    axis.line = element_line(color = "black"),
    panel.background = element_rect(fill = "white", color = NA)
  )

print(p_ext2)
ggsave("p_ext6.png", width = 5, height = 5, dpi = 300, path = "../Output")
ggsave("p_ext6.pdf", width = 5, height = 5, dpi = 300, path = "../Output")

```

## Generate Figure x

```{r message=FALSE}

fig2c <- exp1graph 
fig2c
ggsave("fig2c.pdf", device = pdf, height= 4, width=5, dpi = 300, path = "../Output")
ggsave("fig2c.png", device = png, height= 4, width=5, dpi = 300, path = "../Output")

fig2d <- ext1graph
fig2d
ggsave("fig2d.pdf", device = pdf, height= 4, width=5, dpi = 300, path = "../Output")
ggsave("fig2d.png", device = png, height= 4, width=5, dpi = 300, path = "../Output")

fig3b <- exp3graph
fig3b
ggsave("fig3b.pdf", device = pdf, height= 4, width=5, dpi = 300, path = "../Output")
ggsave("fig3b.png", device = png, height= 4, width=5, dpi = 300, path = "../Output")

fig4b <- congraph
fig4b
ggsave("fig4b.pdf", device = pdf, height= 4, width=5, dpi = 300, path = "../Output")
ggsave("fig4b.png", device = png, height= 4, width=5, dpi = 300, path = "../Output")

fig5d <- trngraph
fig5d
ggsave("fig5d.pdf", device = pdf, height= 4, width=5, dpi = 300, path = "../Output")
ggsave("fig5d.png", device = png, height= 4, width=5, dpi = 300, path = "../Output")

fig5e <- ext2graph
fig5e
ggsave("fig5e.pdf", device = pdf, height= 4, width=5, dpi = 300, path = "../Output")
ggsave("fig5e.png", device = png, height= 4, width=5, dpi = 300, path = "../Output")

trngraph2 
ggsave("figSuppx.pdf", device = pdf, height= 5, width=5, dpi = 300, path = "../Output")
ggsave("figSuppx.png", device = png, height= 5, width=5, dpi = 300, path = "../Output")

```

# Checking Model fit

```{r warning=FALSE, error = FALSE, message=FALSE}

models <- list(
  EXP1_7v9_pooled = lm7,
  EXP1_9v11_pooled = lm8,
  EXP1_13v15_pooled = lm9,
  EXP2_7v9_pooled = lm18,
  EXP2_9v11_pooled = lm19,
  EXP2_13v15_pooled = lm20,
  EXP2_NEGCONT_pooled = lm21,
  EEcon3v1 = clm1,
  EEcon3v2 = clm2,
  EEcon3v4 = clm3,
  EEcon3v5 = clm4,
  SAcon3v1 = clm5,
  SAcon3v2 = clm6,
  SAcon3v4 = clm7,
  SAcon3v5 = clm8,
  training_learning = tlm1,
  training_7v9 = tlm2,
  training_7v8 = tlm3,
  training_EEvsSA_test = tlm_stimuli,
  lm_ext1 = lm_ext1,
  lm_ext2 = lm_ext2,
  lm_ext3 = lm_ext3,
  lm_ext4 = lm_ext4,
  lm_ext4_prime = lm_ext4_prime,
  lm_ext4_composite = lm_ext4_composite,
  lm_ext5 = lm_ext5,
  lm_ext6 = lm_ext6,
  lm_ext6_prime = lm_ext6_prime,
  lm_ext6_composite = lm_ext6_composite,
  lm_trainingphase1 = lm_trainingphase1,
  lm_trainingphase2 = lm_trainingphase2,
  lm_ext_training = lm_ext_training,
  lm_ext_training2 = lm_ext_training2
)

# Check for overdispersion
for (model_name in names(models)) {
  cat(paste0("Overdispersion Check for ", model_name, "\n"))
  print(check_overdispersion(models[[model_name]]))
  cat("\n")
}

# Compute performance metrics for each model
performance_results <- lapply(models, model_performance)

# Capture and print markdown output for each model
for (model_name in names(performance_results)) {
  cat(paste0("### Performance Metrics for ", model_name, "\n\n"))
  
  # Capture the output in markdown format
  output <- capture.output(display(performance_results[[model_name]], format = "markdown", digits = 2, caption = NULL))
  
  # Print the captured output to the console
  cat(paste(output, collapse = "\n"))
  
  cat("\n\n")
}

# Loop for simulating residuals
for (model_name in names(models)) {
  cat(paste0("Simulated Residuals and Plot for ", model_name, "\n"))
  
  # Simulate residuals
  simulated_res <- simulateResiduals(fittedModel = models[[model_name]])
  
  # Plot simulated residuals
  plot(simulated_res)
  
  cat("\n\n")
}

```

